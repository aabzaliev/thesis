a[1,2] <- a[2,1] <- u%*%Q%*%p
a[2,2] <- p%*%Q%*%p
View(a)
d <- a[1,1]*a[2,2] - a[1,2]*a[1,2]
f <- (Q%*%( a[2,2]*u - a[1,2]*p))/d
g <- (Q%*%(-a[1,2]*u + a[1,1]*p))/d
r <- seq(0, Rmax, length=M)
w <- matrix(rep(0,J*M), nrow=J)
for(m in 1:M) w[,m] <- f + r[m]*g
s <- sqrt( a[1,1]*((r - a[1,2]/a[1,1])^2)/d + 1/a[1,1])
ss <- sqrt(diag(S))
minp <- c(sqrt(1/a[1,1]), a[1,2]/a[1,1])
wminp <- f + (a[1,2]/a[1,1])*g
wminp
weightsQP
all.equal(as.numeric(var(returns[[1]] %*% solQP$solution)), as.numeric(2 * solQP$value))
cov(dat$GOOGL, dat$GOOG)
corr(dat$GOOGL, dat$GOOG)
corr(c(dat$GOOGL, dat$GOOG))
corr(cbind(dat$GOOGL, dat$GOOG))
cor(dat$GOOGL, dat$GOOG)
setwd("C:/Users/ngnt/Dropbox/master thesis/code/scripts")
#number of portfolios on frontier
N <- 100
#Maximum daily return value considered
Rmax <- 0.01
data <- read.table(paste("./data/", "portfolio.txt", sep=""),
header=TRUE, sep=",", na.strings = "NA")
data <- data[,colSums(is.na(data))<nrow(data)]
#only since Frebruary
data$KMI <- NULL
data$GOOG <- NULL
is.na(data[[1]])
data <- list(data[, 2:length(data)], as.integer(N),
as.double(Rmax))
dat <- data[[1]]
#number of observations (one less since we make returns and first observation is dropped)
N <- nrow(dat) - 1
#number of stocks
J <- ncol(dat)
ret <- (dat[1:N,1] - dat[2:(N+1),1])/dat[2:(N+1),1];
if(J > 1){
for(j in 2:J){
ret <- cbind(ret, (dat[1:N,j] - dat[2:(N+1),j])/
dat[2:(N+1),j])
}
}
#stupid me - we have time series and we shouldnt use random sampling for CV
#train=sample(seq(T-1),T-50,replace=FALSE)
ret <- ret[,colSums(is.na(ret))<nrow(ret)]
#FB and FBBV and PYPL was not presented in the index in 2011
# KMI appeared only in February
returns <- list(ret, names(data[[1]]), data[[2]], data[[3]])
require("quadprog")
bVec <- 1
zeros <- array(0, dim = c(J,1))
#meq=1 means equality
#calculate covariance matrix
S <- cov(returns[[1]])
p <- colMeans(returns[[1]])
Amat <- matrix(1, nrow=nrow(S))
solQP <- solve.QP(S, zeros, Amat, bVec, meq = 1)
weightsQP <- solQP$solution
all.equal(as.numeric(var(returns[[1]] %*% solQP$solution)), as.numeric(2 * solQP$value))
solQP <- solve.QP(S, zeros, Amat, bVec, meq = 1)
weightsQP
require("quadprog")
bVec <- 1
zeros <- array(0, dim = c(J,1))
#meq=1 means equality
#calculate covariance matrix
S <- cov(returns[[1]])
p <- colMeans(returns[[1]])
Amat <- matrix(1, nrow=nrow(S))
solQP <- solve.QP(S, zeros, Amat, bVec, meq = 1)
weightsQP <- solQP$solution
all.equal(as.numeric(var(returns[[1]] %*% solQP$solution)), as.numeric(2 * solQP$value))
A = as.matrix (expand.grid (rep (list (c(-1,1)), J)))
#add 1 for sum of the coeffcients is equal to 1 constraint
Amat = cbind(1, t(A))
lambda <- 1
# the same story for beta - why is it -1? I guess it should be written in lagrange form
b = rep (-lambda, 2^J)
b = c(1, b)
solLasso <- solve.QP(S, zeros, Amat, b, meq=1)
weightsQP <- solLasso$solution
2^J
bVec <- 1
zeros <- array(0, dim = c(J,1))
Amat <- matrix(1, nrow=nrow(S))
solQP <- solve.QP(S, zeros, Amat, bVec, meq = 1)
weightsQP <- solQP$solution
#or probably better to use analytical solution
wminp <- f + (a[1,2]/a[1,1])*g #TODO implement just with one formula from the data
#our starting weights obtained without constraint
w <- wminp
#constraint all weights to sum up to 1
Amat <- matrix(1, nrow=nrow(S))
bVec <- 1
# use a tolerance on the 1-Norm insted of hard-comparison
# this allows algorithm to be linear vs. exponential in the number of variables
optTol <- as.double(1e-5)
iter <- 0
# while the L1-norm of the parameters is above t
maxIter <- 1000
while (sum(abs(w)) >= lambda + optTol && iter < maxIter) {
iter = iter +1
# we get signs of w, and then in constraint they are multiplied with w itself:
# obviously, +*+ will be plus and -*- will be plus: therefore it is equal to absolute value
#if they together more than lambda, then magic happens and coefficients go down gradually
Amat <- cbind(Amat, sign(w))
bVec <- c(bVec, lambda)
# important: this is negative, since solve.QP solves for constraint A%*%w >= b;
# however, in our case we have less than(i.e. A%*%w <= b) and therefore
# both A and b have to be multiplied by (-1): -A%*%w <= -b
w <- solve.QP(S, zeros, -Amat, -bVec, meq = 1)$solution
}
View(wminp)
setwd("C:/Users/ngnt/Dropbox/master thesis/code/scripts")
#number of portfolios on frontier
N <- 100
#Maximum daily return value considered
Rmax <- 0.01
data <- read.table(paste("./data/", "portfolio.txt", sep=""),
header=TRUE, sep=",", na.strings = "NA")
data <- data[,colSums(is.na(data))<nrow(data)]
#only since Frebruary
data$KMI <- NULL
data$GOOG <- NULL
is.na(data[[1]])
data <- list(data[, 2:length(data)], as.integer(N),
as.double(Rmax))
dat <- data[[1]]
#number of observations (one less since we make returns and first observation is dropped)
N <- nrow(dat) - 1
#number of stocks
J <- ncol(dat)
ret <- (dat[1:N,1] - dat[2:(N+1),1])/dat[2:(N+1),1];
if(J > 1){
for(j in 2:J){
ret <- cbind(ret, (dat[1:N,j] - dat[2:(N+1),j])/
dat[2:(N+1),j])
}
}
#stupid me - we have time series and we shouldnt use random sampling for CV
#train=sample(seq(T-1),T-50,replace=FALSE)
ret <- ret[,colSums(is.na(ret))<nrow(ret)]
#FB and FBBV and PYPL was not presented in the index in 2011
# KMI appeared only in February
returns <- list(ret, names(data[[1]]), data[[2]], data[[3]])
############################################################
p <- colMeans(returns[[1]])
names(p) <- returns[[2]]
J <- ncol(returns[[1]])
M <- returns[[3]]
Rmax <- returns[[4]]
#calculate covariance matrix
S <- cov(returns[[1]])
anyNA(Q)
#inverse of S
Q <- solve(S)
#create unit vector
u <- rep(1,J)
#initialize A-matrix, empty
a <- matrix(rep(0,4),nrow=2)
#populate with values
# If both are vectors of the same length, it will return the inner product (as a matrix)
# therefore we dont need transpose in second multiplication
a[1,1] <- u%*%Q%*%u
a[1,2] <- a[2,1] <- u%*%Q%*%p
a[2,2] <- p%*%Q%*%p
#find the determinant of the matrix
d <- a[1,1]*a[2,2] - a[1,2]*a[1,2]
f <- (Q%*%( a[2,2]*u - a[1,2]*p))/d
g <- (Q%*%(-a[1,2]*u + a[1,1]*p))/d
#generate a sequence of returns to be displayed
r <- seq(0, Rmax, length=M)
#initialize weight matrix
w <- matrix(rep(0,J*M), nrow=J)
#find all the weights for all given levels of return - frontier portfolios
for(m in 1:M) w[,m] <- f + r[m]*g
#find all the variances of the portfolios, for a given return
s <- sqrt( a[1,1]*((r - a[1,2]/a[1,1])^2)/d + 1/a[1,1])
#why do we extract variances of the assets - to populate the matrix with eigenvalues later
ss <- sqrt(diag(S))
# risk-return values of minimum variance portfolio
minp <- c(sqrt(1/a[1,1]), a[1,2]/a[1,1])
#weights of the minumum variance portfolio
wminp <- f + (a[1,2]/a[1,1])*g
#risk-return value of the tangency portfolio
tanp <- c(sqrt(a[2,2])/a[1,2], a[2,2]/a[1,2])
#weights of the tangency portfolio
wtanp <- f + (a[2,2]/a[1,2])*g
#now the part for eigen-portfolios
#big omega matrix
omega <- sqrt(diag(1.0/ss))
#inverse ith
#omega <- solve(omega)
#calculate eigenvalues and eigenvectors of the function
x <- eigen(omega%*%S%*%omega)
#divide each eigenvector by volatility of the corresponding asset - almost weights
v <- omega%*%x$vec
#...and normalize by imposing a constant invested wealth - weights of the eigen-portfolios
#intuition: the weight of a given asset is inversely proportional to its volatility
# "Thus, any portfolio can be represented as a linear combination
# of the eigen-portfolios, since they are orthogonal and
# form a basis in the asset space"
for(j in 1:J) v[,j] <- v[,j]/(u%*%v[,j])
#this is with regularization
sv <- rv <- rep(0, J)
for(j in 1:J){
rv[j] <- t(v[,j])%*%p;
if(rv[j] < 0){
rv[j] <- -rv[j];
v[,j] <- -v[,j];
}
sv[j] <- sqrt(t(v[,j])%*%S%*%v[,j]);
}
bVec <- 1
zeros <- array(0, dim = c(J,1))
Amat <- matrix(1, nrow=nrow(S))
solQP <- solve.QP(S, zeros, Amat, bVec, meq = 1)
weightsQP <- solQP$solution
#or probably better to use analytical solution
wminp <- f + (a[1,2]/a[1,1])*g #TODO implement just with one formula from the data
#our starting weights obtained without constraint
w <- wminp
#constraint all weights to sum up to 1
Amat <- matrix(1, nrow=nrow(S))
bVec <- 1
# use a tolerance on the 1-Norm insted of hard-comparison
# this allows algorithm to be linear vs. exponential in the number of variables
optTol <- as.double(1e-5)
iter <- 0
# while the L1-norm of the parameters is above t
maxIter <- 1000
while (sum(abs(w)) >= lambda + optTol && iter < maxIter) {
iter = iter +1
# we get signs of w, and then in constraint they are multiplied with w itself:
# obviously, +*+ will be plus and -*- will be plus: therefore it is equal to absolute value
#if they together more than lambda, then magic happens and coefficients go down gradually
Amat <- cbind(Amat, sign(w))
bVec <- c(bVec, lambda)
# important: this is negative, since solve.QP solves for constraint A%*%w >= b;
# however, in our case we have less than(i.e. A%*%w <= b) and therefore
# both A and b have to be multiplied by (-1): -A%*%w <= -b
w <- solve.QP(S, zeros, -Amat, -bVec, meq = 1)$solution
}
w
w[w>0.000003]
sum(w[w>0.000003])
y <- returns[[1]][ ,1L]
# compute excess returns
X <- returns[[1]][ ,1L] - returns[[1]][ ,2:J]
solR <- lm(y ~ X)
#the coefficient, except intercept are the weights: so we have replace first coeffcient
# by $1 - sum of all the other coefficients$
cat("weights from qp\n")
as.vector(solQP$solution)
cat("\nweights from regression\n")
as.vector(c(1 - sum(coef(solR)[-1L]), coef(solR)[-1L]))
x <- solve(cov(returns[[1]]), numeric(J) + 1)
x <- x / sum(x)
x
dat <- log(data[[1]])
M <- nrow(dat)
J <- ncol(dat)
ymax = max(dat)
ymin = min(dat)
mycolors <- rainbow(J+1)
dat <- log(data[[1]])
M <- nrow(dat)
J <- ncol(dat)
ymax = max(dat)
ymin = min(dat)
mycolors <- rainbow(J+1)
f <- t(w)
#now weights plot
plot(s, f[,1], xlim=c(0,max(s)), ylim=c(min(f),max(f)),
col=mycolors[1], type="l",
xlab="risk", ylab="portfolio weights",
main = "Efficient Portfolio Weights")
if(J > 1){
for(j in 2:J){
lines(s, f[,j], type="l", col=mycolors[j]);
}
}
abline(h=0, lty=2); abline(v=minp[1], lty=3)
abline(v=tanp[1], lty=3)
text(minp[1], min(f), pos=4, cex=0.5, "MVP1")
text(tanp[1], min(f), pos=4, cex=0.5, "TGP")
legend("topleft", names(p), cex=0.5, pch=rep(15, J),
col=mycolors)
#this is quite strange - the original code used f transposed insted of w
f <- w
#now weights plot
plot(s, f[,1], xlim=c(0,max(s)), ylim=c(min(f),max(f)),
col=mycolors[1], type="l",
xlab="risk", ylab="portfolio weights",
main = "Efficient Portfolio Weights")
if(J > 1){
for(j in 2:J){
lines(s, f[,j], type="l", col=mycolors[j]);
}
}
abline(h=0, lty=2); abline(v=minp[1], lty=3)
abline(v=tanp[1], lty=3)
text(minp[1], min(f), pos=4, cex=0.5, "MVP1")
text(tanp[1], min(f), pos=4, cex=0.5, "TGP")
legend("topleft", names(p), cex=0.5, pch=rep(15, J),
col=mycolors)
y <- returns[[1]][ ,1L]
# compute excess returns
X <- returns[[1]][ ,1L] - returns[[1]][ ,2:J]
install.packages("glmnet")
library("glmnet")
fit.lasso=glmnet(x,y, lambda=1)
y <- returns[[1]][ ,1L]
# compute excess returns
X <- returns[[1]][ ,1L] - returns[[1]][ ,2:J]
fit.lasso=glmnet(X,y, , lambda=1)
plot(fit.lasso,xvar="lambda",label=TRUE)
library(ISLR)
summary(Hitters)
x=model.matrix(Salary~.-1,data=Hitters)
y=Hitters$Salary
View(x)
View(x)
y=Hitters$Salary
fit.lasso=glmnet(x,y)
x=model.matrix(Salary~.-1,data=Hitters)
y=Hitters$Salary
```
x=model.matrix(Salary~.-1,data=Hitters)
y=Hitters$Salary
fit.lasso=glmnet(x,y)
fit.ridge=glmnet(x,y,alpha=0)
plot(fit.ridge,xvar="lambda",label=TRUE)
cv.ridge=cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
library(glmnet)
x=model.matrix(Salary~.-1,data=Hitters)
y=Hitters$Salary
attach(Hitters)
x=model.matrix(Salary~.-1,data=Hitters)
y=Hitters$Salary
Hitters
Model Selection
================
This is an R Markdown document. Markdown is a simple formatting syntax for authoring web pages,
and a very nice way of distributing an analysis. It has some very simple syntax rules.
```{r}
library(ISLR)
summary(Hitters)
```
There are some missing values here, so before we proceed we will remove them:
```{r}
Hitters=na.omit(Hitters)
with(Hitters,sum(is.na(Salary)))
```
Best Subset regression
------------------------
We will now use the package `leaps` to evaluate all the best-subset models.
```{r}
library(leaps)
regfit.full=regsubsets(Salary~.,data=Hitters)
summary(regfit.full)
```
It gives by default best-subsets up to size 8; lets increase that to 19, i.e. all the variables
```{r}
regfit.full=regsubsets(Salary~.,data=Hitters, nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],pch=20,col="red")
```
There is a plot method for the `regsubsets`  object
```{r}
plot(regfit.full,scale="Cp")
coef(regfit.full,10)
```
Forward Stepwise Selection
--------------------------
Here we use the `regsubsets` function but specify the `method="forward" option:
```{r}
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
summary(regfit.fwd)
plot(regfit.fwd,scale="Cp")
```
Model Selection Using a Validation Set
---------------------------------------
Lets make a training and validation set, so that we can choose a good subset model.
We will do it using a slightly different approach from what was done in the the book.
```{r}
dim(Hitters)
set.seed(1)
train=sample(seq(263),180,replace=FALSE)
train
regfit.fwd=regsubsets(Salary~.,data=Hitters[train,],nvmax=19,method="forward")
```
Now we will make predictions on the observations not used for training. We know there are 19 models, so we set up some vectors to record the errors. We have to do a bit of work here, because there is no predict method for `regsubsets`.
```{r}
val.errors=rep(NA,19)
x.test=model.matrix(Salary~.,data=Hitters[-train,])# notice the -index!
for(i in 1:19){
coefi=coef(regfit.fwd,id=i)
pred=x.test[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
plot(sqrt(val.errors),ylab="Root MSE",ylim=c(300,400),pch=19,type="b")
points(sqrt(regfit.fwd$rss[-1]/180),col="blue",pch=19,type="b")
legend("topright",legend=c("Training","Validation"),col=c("blue","black"),pch=19)
```
As we expect, the training error goes down monotonically as the model gets bigger, but not so
for the validation error.
This was a little tedious - not having a predict method for `regsubsets`. So we will write one!
```{r}
predict.regsubsets=function(object,newdata,id,...){
form=as.formula(object$call[[2]])
mat=model.matrix(form,newdata)
coefi=coef(object,id=id)
mat[,names(coefi)]%*%coefi
}
```
Model Selection by Cross-Validation
-----------------------------------
We will do 10-fold cross-validation. Its really easy!
```{r}
set.seed(11)
folds=sample(rep(1:10,length=nrow(Hitters)))
folds
table(folds)
cv.errors=matrix(NA,10,19)
for(k in 1:10){
best.fit=regsubsets(Salary~.,data=Hitters[folds!=k,],nvmax=19,method="forward")
for(i in 1:19){
pred=predict(best.fit,Hitters[folds==k,],id=i)
cv.errors[k,i]=mean( (Hitters$Salary[folds==k]-pred)^2)
}
}
rmse.cv=sqrt(apply(cv.errors,2,mean))
plot(rmse.cv,pch=19,type="b")
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
summary(regfit.fwd)
plot(regfit.fwd,scale="Cp")
dim(Hitters)
set.seed(1)
train=sample(seq(263),180,replace=FALSE)
train
regfit.fwd=regsubsets(Salary~.,data=Hitters[train,],nvmax=19,method="forward")
val.errors=rep(NA,19)
x.test=model.matrix(Salary~.,data=Hitters[-train,])# notice the -index!
for(i in 1:19){
coefi=coef(regfit.fwd,id=i)
pred=x.test[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
plot(sqrt(val.errors),ylab="Root MSE",ylim=c(300,400),pch=19,type="b")
points(sqrt(regfit.fwd$rss[-1]/180),col="blue",pch=19,type="b")
legend("topright",legend=c("Training","Validation"),col=c("blue","black"),pch=19)
predict.regsubsets=function(object,newdata,id,...){
form=as.formula(object$call[[2]])
mat=model.matrix(form,newdata)
coefi=coef(object,id=id)
mat[,names(coefi)]%*%coefi
}
set.seed(11)
folds=sample(rep(1:10,length=nrow(Hitters)))
folds
table(folds)
cv.errors=matrix(NA,10,19)
for(k in 1:10){
best.fit=regsubsets(Salary~.,data=Hitters[folds!=k,],nvmax=19,method="forward")
for(i in 1:19){
pred=predict(best.fit,Hitters[folds==k,],id=i)
cv.errors[k,i]=mean( (Hitters$Salary[folds==k]-pred)^2)
}
}
rmse.cv=sqrt(apply(cv.errors,2,mean))
plot(rmse.cv,pch=19,type="b")
library(glmnet)
x=model.matrix(Salary~.-1,data=Hitters)
y=Hitters$Salary
fit.ridge=glmnet(x,y,alpha=0)
plot(fit.ridge,xvar="lambda",label=TRUE)
fit.ridge$beta
fit.lasso=glmnet(X,y,lambda=1)
#generate one asset as regressand
y <- returns[[1]][ ,1L]
# compute excess returns
X <- returns[[1]][ ,1L] - returns[[1]][ ,2:J]
solR <- lm(y ~ X)
fit.lasso=glmnet(X,y,lambda=1)
fit.lasso$beta
fit.lasso=glmnet(y,X,lambda=1)
fit.lasso=glmnet(X,y)
fit.lasso$beta
View(wminp)
fit.lasso=glmnet(X,y)
fit.lasso$beta[1]
fit.lasso$beta[2]
fit.lasso$beta[[1]]
fit.lasso$beta
View(wminp)
y <- returns[[1]][ ,1L]
# compute excess returns
X <- returns[[1]][ ,1L] - returns[[1]][ ,2:J]
solR <- lm(y ~ X)
fit.lasso=glmnet(X,y, lambda=0)
fit.lasso$beta
View(wminp)
